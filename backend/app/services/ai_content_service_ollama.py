"""
AI Content Generator Service - Ollama Version
Generate memes, captions using Ollama (local) or OpenAI (cloud)
"""

import requests
import json
from typing import Dict, List
import random

from app.core.config import settings

class AIContentGenerator:
    """Generate engaging football content using AI"""

    def __init__(self):
        self.use_ollama = settings.USE_OLLAMA

        if self.use_ollama:
            # Ollama local
            self.ollama_url = settings.OLLAMA_BASE_URL
            self.model = settings.OLLAMA_MODEL
            print(f"Using Ollama: {self.model} at {self.ollama_url}")
        else:
            # OpenAI cloud
            from openai import OpenAI
            self.client = OpenAI(api_key=settings.OPENAI_API_KEY) if settings.OPENAI_API_KEY else None
            self.model = settings.OPENAI_MODEL
            if self.client:
                print(f"‚úÖ Using OpenAI: {self.model}")
            else:
                print("‚ö†Ô∏è No AI configured, using fallback mode")

        self.meme_styles = ["sarcastic", "funny", "emotional", "dramatic", "wholesome"]

    async def generate_meme_caption(self, news_title: str, news_description: str, style: str = "funny") -> Dict:
        """Generate a meme caption based on news"""

        # Style-specific instructions
        style_prompts = {
            "funny": "T·∫°o caption H√ÄI H∆Ø·ªöC, ch∆°i ch·ªØ, troll nh·∫π nh√†ng. D√πng ng√¥n ng·ªØ Gen Z, meme culture.",
            "sarcastic": "T·∫°o caption CH√ÇM BI·∫æM, m·ªâa mai tinh t·∫ø. D√πng gi·ªçng ƒëi·ªáu 'fake support' ho·∫∑c 'ironic praise'.",
            "dramatic": "T·∫°o caption K·ªäCH T√çNH, ph√≥ng ƒë·∫°i, nh∆∞ b√¨nh lu·∫≠n vi√™n h√†o h·ª©ng. D√πng CAPS, d·∫•u ch·∫•m than!!!",
            "emotional": "T·∫°o caption C·∫¢M ƒê·ªòNG, t√¥n vinh tinh th·∫ßn th·ªÉ thao. Nghi√™m t√∫c nh∆∞ng g·∫ßn g≈©i.",
            "savage": "T·∫°o caption TH·∫≤NG TH·∫ÆN, troll m·∫°nh tay, roast kh√¥ng th∆∞∆°ng ti·∫øc. D√πng 'b√≥c ph·ªët' style."
        }

        style_instruction = style_prompts.get(style, style_prompts["funny"])

        prompt = f"""B·∫°n l√† chuy√™n gia t·∫°o meme b√≥ng ƒë√° ti·∫øng Vi·ªát.

TIN T·ª®C:
Ti√™u ƒë·ªÅ: {news_title}
Chi ti·∫øt: {news_description}

PHONG C√ÅCH Y√äU C·∫¶U: {style}
{style_instruction}

H√ÉY T·∫†O:
1. Caption ch√≠nh (1-2 c√¢u, ph√π h·ª£p phong c√°ch {style})
2. Top text (text ng·∫Øn cho ph·∫ßn tr√™n ·∫£nh meme, 3-8 t·ª´)
3. Bottom text (text ng·∫Øn cho ph·∫ßn d∆∞·ªõi ·∫£nh meme, 3-8 t·ª´)
4. 5-7 hashtags (trending, c√≥ bi·∫øn th·ªÉ ch∆°i ch·ªØ)
5. Emoji ph√π h·ª£p

Y√äU C·∫¶U:
- Ng√¥n ng·ªØ: Ti·∫øng Vi·ªát Gen Z, t·ª± nhi√™n
- Caption: Ng·∫Øn g·ªçn, d·ªÖ hi·ªÉu, viral potential
- Hashtags: Mix serious (#BongDa, #PremierLeague) + funny (#TrollFC, #MemeBongDa)
- Top/Bottom text: VI·∫æT HOA, ng·∫Øn, bold, quotable

QUAN TR·ªåNG: Tr·∫£ v·ªÅ ƒê√öNG format JSON n√†y, KH√îNG th√™m text n√†o kh√°c:
{{
  "caption": "Caption ch√≠nh ·ªü ƒë√¢y",
  "top_text": "TOP TEXT NG·∫ÆN G·ªåN",
  "bottom_text": "BOTTOM TEXT NG·∫ÆN G·ªåN",
  "meme_text_top": "TOP TEXT NG·∫ÆN G·ªåN",
  "meme_text_bottom": "BOTTOM TEXT NG·∫ÆN G·ªåN",
  "hashtags": ["#BongDa", "#TrollFC", "#MemeBongDa", "#PremierLeague", "#Viral"],
  "emoji": "üòÇ"
}}
"""

        if self.use_ollama:
            return await self._generate_with_ollama(prompt, news_title)
        elif hasattr(self, 'client') and self.client:
            return await self._generate_with_openai(prompt)
        else:
            return self._generate_fallback_caption(news_title)

    async def _generate_with_ollama(self, prompt: str, news_title: str) -> Dict:
        """Generate content using Ollama local"""
        try:
            response = requests.post(
                f"{self.ollama_url}/api/generate",
                json={
                    "model": self.model,
                    "prompt": prompt,
                    "stream": False,
                    "format": "json"
                },
                timeout=60
            )

            if response.status_code == 200:
                result = response.json()
                content = result.get("response", "")

                # Parse JSON t·ª´ response
                try:
                    parsed = json.loads(content)
                    return parsed
                except json.JSONDecodeError:
                    print(f"‚ö†Ô∏è Ollama response kh√¥ng ph·∫£i JSON h·ª£p l·ªá: {content[:200]}")
                    return self._generate_fallback_caption(news_title)
            else:
                print(f"‚ùå Ollama error: {response.status_code}")
                return self._generate_fallback_caption(news_title)

        except requests.exceptions.ConnectionError:
            print("‚ùå Kh√¥ng k·∫øt n·ªëi ƒë∆∞·ª£c Ollama. Ch·∫°y: ollama serve")
            return self._generate_fallback_caption(news_title)
        except Exception as e:
            print(f"‚ùå Ollama generation error: {str(e)}")
            return self._generate_fallback_caption(news_title)

    async def _generate_with_openai(self, prompt: str) -> Dict:
        """Generate content using OpenAI"""
        try:
            response = self.client.chat.completions.create(
                model=self.model,
                messages=[
                    {"role": "system", "content": "B·∫°n l√† chuy√™n gia t·∫°o n·ªôi dung meme b√≥ng ƒë√° ti·∫øng Vi·ªát."},
                    {"role": "user", "content": prompt}
                ],
                temperature=0.8,
                max_tokens=500,
                response_format={"type": "json_object"}
            )

            result = json.loads(response.choices[0].message.content)
            return result

        except Exception as e:
            print(f"‚ùå OpenAI generation error: {str(e)}")
            return self._generate_fallback_caption("")

    async def generate_interactive_post(self, topic: str = None) -> Dict:
        """Generate interactive content (polls, questions, debates)"""

        prompt = f"""T·∫°o b√†i ƒëƒÉng t∆∞∆°ng t√°c cho fanpage b√≥ng ƒë√° Vi·ªát Nam.

Ch·ªß ƒë·ªÅ: {topic if topic else "b√≥ng ƒë√° n√≥i chung"}

H√£y t·∫°o 1 trong c√°c d·∫°ng sau:
1. C√¢u h·ªèi tranh lu·∫≠n (debate)
2. B√¨nh ch·ªçn (poll)
3. Game ƒëo√°n (quiz)

Y√™u c·∫ßu:
- G√¢y t∆∞∆°ng t√°c cao
- Vui nh·ªôn, kh√¥ng nghi√™m t√∫c qu√°
- D·ªÖ tr·∫£ l·ªùi/b√¨nh lu·∫≠n

QUAN TR·ªåNG: Tr·∫£ v·ªÅ ƒê√öNG format JSON n√†y:
{{
  "type": "poll",
  "content": "N·ªôi dung b√†i ƒëƒÉng",
  "options": ["A", "B", "C"],
  "hashtags": ["#tag1", "#tag2"]
}}
"""

        if self.use_ollama:
            try:
                response = requests.post(
                    f"{self.ollama_url}/api/generate",
                    json={
                        "model": self.model,
                        "prompt": prompt,
                        "stream": False,
                        "format": "json"
                    },
                    timeout=60
                )

                if response.status_code == 200:
                    result = response.json()
                    content = result.get("response", "")
                    return json.loads(content)

            except Exception as e:
                print(f"‚ùå Error: {str(e)}")

        return self._generate_fallback_interactive()

    async def generate_daily_content_ideas(self, count: int = 5) -> List[Dict]:
        """Generate content ideas for the day - SIMPLIFIED"""
        # ƒê∆°n gi·∫£n h√≥a: tr·∫£ v·ªÅ ideas c·ªë ƒë·ªãnh thay v√¨ g·ªçi AI
        return self._generate_fallback_ideas(count)

    async def improve_caption(self, original_caption: str) -> str:
        """Improve existing caption to make it more engaging"""

        if self.use_ollama:
            prompt = f"""C·∫£i thi·ªán caption n√†y cho fanpage b√≥ng ƒë√°:

"{original_caption}"

Y√™u c·∫ßu:
- Gi·ªØ √Ω ch√≠nh
- Th√™m y·∫øu t·ªë vui nh·ªôn/troll nh·∫π
- Ng·∫Øn g·ªçn h∆°n n·∫øu c·∫ßn
- Th√™m emoji ph√π h·ª£p

Ch·ªâ tr·∫£ v·ªÅ caption ƒë√£ c·∫£i thi·ªán, kh√¥ng th√™m text kh√°c.
"""

            try:
                response = requests.post(
                    f"{self.ollama_url}/api/generate",
                    json={
                        "model": self.model,
                        "prompt": prompt,
                        "stream": False
                    },
                    timeout=30
                )

                if response.status_code == 200:
                    result = response.json()
                    return result.get("response", original_caption).strip()

            except Exception as e:
                print(f"‚ùå Error: {str(e)}")

        return original_caption

    def _generate_fallback_caption(self, news_title: str) -> Dict:
        """Fallback when AI is not available"""
        emojis = ["üòÇ", "üò≠", "üíÄ", "üî•", "‚öΩ", "ü§°", "üëÄ", "üòÖ"]

        return {
            "caption": f"{news_title} {random.choice(emojis)}",
            "meme_text_top": news_title[:50],
            "meme_text_bottom": "AI ƒëang b·∫£o tr√¨ üòÖ",
            "hashtags": ["#BongDa", "#MemeBongDa", "#TrollFC"],
            "emoji": random.choice(emojis)
        }

    def _generate_fallback_interactive(self) -> Dict:
        """Fallback interactive content"""
        questions = [
            "ƒê·ªôi b√≥ng y√™u th√≠ch c·ªßa b·∫°n l√†?",
            "C·∫ßu th·ªß n√†o b·∫°n mu·ªën v·ªÅ MU?",
            "Ai l√† GOAT: Messi hay Ronaldo?",
            "HLV n√†o x·ª©ng ƒë√°ng b·ªã sa th·∫£i nh·∫•t?",
            "ƒê·ªôi n√†o v√¥ ƒë·ªãch Champions League nƒÉm nay?"
        ]

        return {
            "type": "question",
            "content": random.choice(questions),
            "hashtags": ["#BongDa", "#TraiLuan"]
        }

    def _generate_fallback_ideas(self, count: int) -> List[Dict]:
        """Fallback content ideas"""
        ideas = [
            {"type": "meme", "topic": "K·∫øt qu·∫£ tr·∫≠n ƒë·∫•u h√¥m qua", "caption_idea": "Troll nh·∫π ƒë·ªôi thua", "best_time": "08:00", "priority": "high"},
            {"type": "news", "topic": "Tin chuy·ªÉn nh∆∞·ª£ng hot", "caption_idea": "Ph√¢n t√≠ch vui", "best_time": "12:00", "priority": "medium"},
            {"type": "interactive", "topic": "B√¨nh ch·ªçn c·∫ßu th·ªß hay nh·∫•t", "caption_idea": "Ai hay nh·∫•t tu·∫ßn n√†y?", "best_time": "20:00", "priority": "medium"},
            {"type": "meme", "topic": "Highlight h√†i h∆∞·ªõc", "caption_idea": "Top pha b√≥ng l·∫ßy", "best_time": "17:00", "priority": "high"},
            {"type": "news", "topic": "Th·ªëng k√™ th√∫ v·ªã", "caption_idea": "S·ªë li·ªáu b·∫•t ng·ªù", "best_time": "22:00", "priority": "low"}
        ]

        return ideas[:count]

# Singleton instance
ai_generator = AIContentGenerator()
